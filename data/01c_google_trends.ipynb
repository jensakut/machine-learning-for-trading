{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "involved-century",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/using-google-trends-at-scale-1c8b902b6bfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_DEFAULT_ENV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jak/miniconda3/envs/ml4t_gpu\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_PREFIX\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "above-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import monthrange\n",
    "from datetime import timedelta, datetime, date\n",
    "from functools import partial\n",
    "from random import randrange, randint\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from pytrends.exceptions import ResponseError\n",
    "from pytrends.request import TrendReq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_timeframe_daily_resolution(start: date, stop: date) -> str:\n",
    "    \"\"\"Given two dates, returns a string representing the interval between the\n",
    "    dates. This is string is used to retrieve data for a specific time frame\n",
    "    from Google Trends.\n",
    "    \"\"\"\n",
    "    return f\"{start.strftime('%Y-%m-%d')} {stop.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "def get_timeframe_hourly_resolution(start: datetime, stop: datetime) -> str:\n",
    "    \"\"\"Given two datetimes, returns a string representing the interval between the\n",
    "    dates. This is string is used to retrieve data for a specific time frame\n",
    "    from Google Trends.\n",
    "    \"\"\"\n",
    "    return f\"{start.strftime('%Y-%m-%dT%H')} {stop.strftime('%Y-%m-%dT%H')}\"\n",
    "\n",
    "\n",
    "\n",
    "def _fetch_data(pytrends, build_payload, timeframe: str) -> pd.DataFrame:\n",
    "    \"\"\"Attempts to fecth data and retries in case of a ResponseError.\"\"\"\n",
    "    attempts, fetched = 0, False\n",
    "    if randint(0, 10) == 0:\n",
    "        print(\"sleep 60 s\")\n",
    "        sleep(60)\n",
    "\n",
    "    while not fetched:\n",
    "        try:\n",
    "            build_payload(timeframe=timeframe)\n",
    "        except Exception:\n",
    "            wait_time = 300 * attempts  # Start with 0 due to timeouts\n",
    "            print(f'Trying again in {wait_time / 60:.0f} minutes.')\n",
    "            sleep(wait_time)\n",
    "            attempts += 1\n",
    "        else:\n",
    "            result = pytrends.interest_over_time()\n",
    "            if result.empty:\n",
    "                print(\"dataframe empty, sleep 5 min\")\n",
    "                attempts += 1\n",
    "            else:\n",
    "                fetched = True\n",
    "    return pytrends.interest_over_time()\n",
    "\n",
    "\n",
    "def get_hourly_data(search_term: str,\n",
    "                    start_year: int = 2020,\n",
    "                    start_month: int = 1,\n",
    "                    start_day: int = 1,\n",
    "                    stop_year: int = 2021,\n",
    "                    stop_month: int = 12,\n",
    "                    stop_day: int = 31,\n",
    "                    geo: str = '',  # for worldwide aggregation\n",
    "                    tz: int = 0,  # for utc!\n",
    "                    cat: int = 0,\n",
    "                    verbose: bool = True,\n",
    "                    clean: bool = True,\n",
    "                    wait_time: float = 5.0,\n",
    "                    shift_hourly: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Given a search term, fetches daily search volume data from Google Trends and\n",
    "    returns results in a pandas DataFrame.\n",
    "    Details: Due to the way Google Trends scales and returns data, special\n",
    "    care needs to be taken to make the daily data comparable over different\n",
    "    months. To do that, we download daily data on a month by month basis,\n",
    "    and also monthly data. The monthly data is downloaded in one go, so that\n",
    "    the monthly values are comparable amongst themselves and can be used to\n",
    "    scale the daily data. In a given month, the daily data is scaled so that\n",
    "    the month by month average of daily values is equal to the values at the\n",
    "    monthly frequency. That is, the daily data is scaled by multiplying the\n",
    "    daily values by the ratio of the monthly series value to the monthly\n",
    "    average of the daily data.\n",
    "\n",
    "    Args:\n",
    "        search_term (str): search_term to fetch daily data for.\n",
    "        start_year (int): First year to fetch data for. Starts at the beginning\n",
    "            of this year (1st of January).\n",
    "        start_month (int): First month of the first year\n",
    "        start_day (int): First day of the first year\n",
    "        stop_year (int): Last year to fetch data for (inclusive).\n",
    "        stop_month (int): Last month of the last year\n",
    "        stop_day (int): Last day\n",
    "        geo (str): Geographical area code. Default at 'US'.\n",
    "        tz (int): Time zone, minutes offset off GMT (240 for US EST).\n",
    "        cat (int): Category, default 0 for no category. Use trends.google.com and\n",
    "        check the header for more information.\n",
    "        verbose (bool): If True, then prints the word and current time frame\n",
    "            we are fecthing the data for.\n",
    "        clean (bool): If True, clean up the dataframe, else leave information for\n",
    "            diagnostics\n",
    "        wait_time (float): Scaling factor for how much to wait between data\n",
    "            requests. If 0, then a new request is sent at about every 0.5\n",
    "            second. The default of 5 seconds implies in a new request being\n",
    "            sent at about every 3 seconds (random).\n",
    "        shift_hourly (bool): shifts for 3 days which is useful for validation (see tests)\n",
    "    Returns:\n",
    "        complete (pd.DataFrame): Contains 4 columns.\n",
    "            The column named after the word argument contains the daily search\n",
    "            volume already scaled and comparable through time.\n",
    "            The column f'{word}_{geo}_unscaled' is the original daily data\n",
    "            fetched month by month, and it is not comparable across different\n",
    "            months (but is comparable within a month).\n",
    "            The column f'{word}_{geo}_monthly' contains the original monthly\n",
    "            data fetched at once. The values in this column have been\n",
    "            backfilled so that there are no NaN present.\n",
    "            The column 'scale' contains the scale used to obtain the scaled\n",
    "            daily data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up start and stop dates\n",
    "    start_date = datetime(start_year, start_month, start_day)\n",
    "    # stop_date cannot be later than today's date\n",
    "    stop_date = min([datetime(stop_year, stop_month, stop_day), datetime.today()])\n",
    "\n",
    "    # Start pytrends for US region\n",
    "    pytrends = TrendReq(tz=tz)\n",
    "    # Initialize build_payload with the search_term we need data for\n",
    "    build_payload = partial(pytrends.build_payload, kw_list=[search_term], cat=cat, geo=geo, gprop='')\n",
    "\n",
    "\n",
    "    daily = get_daily_gtrends(build_payload, geo, pytrends, search_term, start_date, stop_date, verbose, wait_time)\n",
    "    monthly = get_monthly_gtrends(build_payload, pytrends, search_term, start_date, stop_date, verbose)\n",
    "\n",
    "    daily['interest_daily_monthly_mean'] = daily['interest_daily_raw'].resample('M').mean()\n",
    "    daily['interest_daily_monthly_mean'].bfill(inplace=True)  # Fill in backward because 'monthly' resampling is the\n",
    "    # other way round\n",
    "    monthly_daily_interest = daily.join(monthly)\n",
    "\n",
    "\n",
    "    # fill NaN values\n",
    "    monthly_daily_interest['interest_monthly'].ffill(inplace=True)\n",
    "    # compute month_by_day_scale\n",
    "    monthly_daily_interest['month_day_scale'] = monthly_daily_interest['interest_monthly'] / monthly_daily_interest['interest_daily_monthly_mean']\n",
    "    monthly_daily_interest['interest_daily'] = monthly_daily_interest['interest_daily_raw'] * monthly_daily_interest.month_day_scale\n",
    "\n",
    "    # hourly = get_hourly_gtrends(geo, pytrends, search_term, start_date, stop_date, verbose, wait_time)\n",
    "    # Compute month by month averages of daily data\n",
    "    \"\"\"\n",
    "    if shift_hourly:\n",
    "        hourly = get_hourly_gtrends(build_payload, geo, pytrends, search_term, start_date-timedelta(days=3), stop_date, verbose, wait_time)\n",
    "    else:\n",
    "        hourly = get_hourly_gtrends(build_payload, geo, pytrends, search_term, start_date, stop_date, verbose, wait_time)\n",
    "\n",
    "    hourly['interest_hourly_7D_mean'] = hourly['interest_hourly_raw'].resample('7D').mean()\n",
    "    hourly['interest_hourly_7D_mean'].ffill(inplace=True)  # Fill in forward\n",
    "    interest = hourly.join(monthly_daily_interest)\n",
    "\n",
    "    # Compute 7D mean for hourly usage\n",
    "    interest['interest_daily_7D_mean'] = interest['interest_daily'].resample('7D').mean()\n",
    "    interest['interest_daily_7D_mean'].ffill(inplace=True)  # fill NaN values\n",
    "    # Scale hourly data by 7-day weights so the data is comparable\n",
    "\n",
    "    interest['daily_hourly_scaling'] = interest['interest_daily_7D_mean'] / interest['interest_hourly_7D_mean']\n",
    "    interest['interest_hourly'] = interest['interest_hourly_raw'] * interest.daily_hourly_scaling\n",
    "\n",
    "    interest['interest_monthly'].ffill(inplace=True)\n",
    "    interest['interest_daily'].ffill(inplace=True)\n",
    "    print('hourly mean {}'.format(interest['interest_hourly'].mean()))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if clean:\n",
    "        cols_to_keep = ['interest_monthly',\n",
    "                        'interest_daily',\n",
    "                        #'interest_hourly'\n",
    "                        ]\n",
    "        interest = monthly_daily_interest.loc[:, cols_to_keep]\n",
    "\n",
    "    print('monthly mean {}'.format(interest['interest_monthly'].mean()))\n",
    "    print('daily mean {}'.format(interest['interest_daily'].mean()))\n",
    "\n",
    "    interest['search_term'] = search_term\n",
    "    interest['google_category'] = cat\n",
    "    interest.set_index(['search_term', 'google_category'], inplace=True, append=True)\n",
    "    # interest.stack(['search_term', 'google_category'])\n",
    "    return interest\n",
    "\n",
    "\n",
    "def get_hourly_gtrends(build_payload, geo, pytrends, search_term, start_date, stop_date, verbose, wait_time):\n",
    "    # Get daily data, month by month\n",
    "    results = []\n",
    "    attempts = 0\n",
    "    # If a timeout or too many requests error occur we need to adjust wait time\n",
    "    current = start_date\n",
    "    while current < stop_date:\n",
    "        end_date = current + timedelta(days=7)\n",
    "\n",
    "        timeframe = get_timeframe_hourly_resolution(current, end_date)\n",
    "        if verbose:\n",
    "            print(f'{search_term}/{geo}:{timeframe}')\n",
    "        result = _fetch_data(pytrends, build_payload, timeframe)\n",
    "\n",
    "        result.rename(columns={search_term: 'interest_hourly_raw',\n",
    "                               'isPartial': 'hourly_isPartial'}, inplace=True)\n",
    "        result = result['interest_hourly_raw'].apply(lambda x: max(x, 0.1))\n",
    "\n",
    "        if verbose:\n",
    "            print(result)\n",
    "        results.append(result)\n",
    "        current = end_date + timedelta(hours=1)\n",
    "\n",
    "        # Don't go too fast or Google will send 429s\n",
    "        sleep(randrange(10, round(10 * wait_time)) / 10)\n",
    "\n",
    "    # Concatenate daily data into a single dataframe\n",
    "    daily = pd.concat(results)\n",
    "    return daily\n",
    "\n",
    "\"\"\"\n",
    "def get_hourly_gtrends_old(geo, pytrends, search_term, start_date, stop_date, verbose, wait_time):\n",
    "    # Get hourly data, week by week\n",
    "    results = []\n",
    "    # If a timeout or too many requests error occur we need to adjust wait time\n",
    "    current = start_date\n",
    "    while current < stop_date:\n",
    "        end_date = current + timedelta(days=7) - timedelta(hours=1)\n",
    "        print(f'{search_term} {geo} : {current} to {end_date}')\n",
    "        result = pytrends.get_historical_interest([search_term], year_start=current.year,\n",
    "                                                  month_start=current.month,\n",
    "                                                  day_start=current.day,\n",
    "                                                  hour_start=current.hour,\n",
    "                                                  year_end=end_date.year,\n",
    "                                                  month_end=end_date.month,\n",
    "                                                  day_end=end_date.day,\n",
    "                                                  hour_end=end_date.hour,\n",
    "                                                  cat=0, geo='', gprop='', sleep=0)\n",
    "        result.rename(columns={search_term: 'interest_hourly_raw',\n",
    "                               'isPartial': 'hourly_isPartial'}, inplace=True)\n",
    "\n",
    "        results.append(result)\n",
    "        if verbose:\n",
    "            print(result)\n",
    "        current = current + timedelta(days=7)\n",
    "        # Don't go too fast or Google will send 429s\n",
    "        sleep(randrange(10, round(10 * wait_time)) / 10)\n",
    "    # Concatenate daily data into a single dataframe\n",
    "    hourly = pd.concat(results)\n",
    "    return hourly\n",
    "\"\"\"\n",
    "\n",
    "def get_daily_gtrends(build_payload, geo, pytrends, search_term, start_date, stop_date, verbose, wait_time):\n",
    "    # Get daily data, month by month\n",
    "    results = []\n",
    "    # If a timeout or too many requests error occur we need to adjust wait time\n",
    "    current = start_date\n",
    "    while current < stop_date:\n",
    "        lastDateOfMonth = datetime(current.year, current.month,\n",
    "                                   monthrange(current.year, current.month)[1])\n",
    "        timeframe = get_timeframe_daily_resolution(current, lastDateOfMonth)\n",
    "        print(f'{search_term}/{geo}:{timeframe}')\n",
    "        result = _fetch_data(pytrends, build_payload, timeframe)\n",
    "\n",
    "        result.rename(columns={search_term: 'interest_daily_raw',\n",
    "                               'isPartial': 'daily_isPartial'}, inplace=True)\n",
    "        # result = result['interest_daily_raw'].apply(lambda x: max(x, 0.1))\n",
    "\n",
    "        if verbose:\n",
    "            print(result)\n",
    "        if not result.empty:\n",
    "            results.append(result)\n",
    "            current = lastDateOfMonth + timedelta(days=1)\n",
    "\n",
    "        # Don't go too fast or Google will send 429s\n",
    "        sleep(randrange(10, round(10 * wait_time)) / 10)\n",
    "    # Concatenate daily data into a single dataframe\n",
    "    daily = pd.concat(results)\n",
    "    return daily\n",
    "\n",
    "\n",
    "def get_monthly_gtrends(build_payload, pytrends, search_term, start_date, stop_date, verbose):\n",
    "    # Obtain monthly data for all months in years [2004, stop_year]\n",
    "    monthly = _fetch_data(pytrends, build_payload,\n",
    "                          get_timeframe_daily_resolution(datetime(2004, 1, 1), stop_date))[start_date:stop_date]\n",
    "    monthly.rename(columns={search_term: 'interest_monthly',\n",
    "                            'isPartial': 'monthly_isPartial'}, inplace=True)\n",
    "    monthly = monthly['interest_monthly'].apply(lambda x: max(x, 0.1))\n",
    "    if verbose:\n",
    "        print(monthly)\n",
    "    return monthly"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the coins and coin names and save google trend data for the words and categories "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md \n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "charitable-beverage",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_STORE = '../data/crypto.h5'\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    # todo insert market\n",
    "    market = store['coingecko/top100/market']\n",
    "    # cats = store['coingecko/top100/cats']\n",
    "    # col_list = ['name', 'id']\n",
    "    # market_cut = market.loc[:, col_list]\n",
    "    prices = store['crypto/caggle/prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(Index(['btc', 'ltc', 'eth', 'etc', 'xmr', 'xrp', 'miota', 'eos', 'neo', 'trx',\n        'dai', 'mtn', 'xlm', 'mkr', 'man', 'vet', 'xtz', 'bsv', 'usdt', 'usdc',\n        'btt', 'atom', 'wbtc', 'okb', 'algo', 'ftt', 'doge', 'ada', 'dot',\n        'ksm', 'uni', 'fil', 'sol', 'aave', 'avax', 'bch', 'link', 'luna'],\n       dtype='object', name='symbol'),\n 38)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the list of symbols from prices data\n",
    "prices_symbols = prices.index.get_level_values('symbol').unique()\n",
    "prices_symbols, len(prices_symbols)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crypto/gtrends/btc_gtrend\n"
     ]
    }
   ],
   "source": [
    "def symbol_to_store_path(symbol):\n",
    "    return 'crypto/gtrends/' + symbol + '_gtrend'\n",
    "\n",
    "print(symbol_to_store_path('btc'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                    id    market_cap            name genesis_date  \\\nsymbol                                                              \nbtc            bitcoin  943219001348         Bitcoin   2009-01-03   \neth           ethereum  444995246879        Ethereum   2015-07-30   \nbnb        binancecoin   92441015166    Binance Coin   2017-07-08   \nxrp             ripple   60573262757             XRP         None   \nusdt            tether   58147824102          Tether         None   \n...                ...           ...             ...          ...   \nrvn          ravencoin    1186666191       Ravencoin   2018-01-03   \nar             arweave    1184818634         Arweave         None   \ntusd          true-usd    1177644257         TrueUSD   2018-03-05   \npax     paxos-standard    1172924271  Paxos Standard         None   \niost          iostoken    1128186332            IOST   2018-01-11   \n\n        market_cap_rank hashing_algorithm  coingecko_rank  coingecko_score  \\\nsymbol                                                                       \nbtc                   1           SHA-256               2           79.223   \neth                   2            Ethash               3           77.163   \nbnb                   3              None               5           67.775   \nxrp                   4              None               9           65.306   \nusdt                  5              None             151           41.650   \n...                 ...               ...             ...              ...   \nrvn                  96              x16r              39           54.848   \nar                   96              None             672           25.543   \ntusd                 98              None             131           43.754   \npax                  99              None             174           39.457   \niost                100              None              38           54.955   \n\n        developer_score  community_score  liquidity_score  \\\nsymbol                                                      \nbtc              98.887           65.771          100.158   \neth              97.176           62.612           98.968   \nbnb              73.253           66.032           83.296   \nxrp              71.120           54.187           87.663   \nusdt              0.000           10.676          107.153   \n...                 ...              ...              ...   \nrvn              68.040           46.428           61.649   \nar                0.000            9.027           42.365   \ntusd             62.725            8.702           55.802   \npax              47.971            8.271           52.646   \niost             67.201           45.077           64.822   \n\n        public_interest_score  \nsymbol                         \nbtc                       0.0  \neth                       0.0  \nbnb                       0.0  \nxrp                       0.0  \nusdt                      0.0  \n...                       ...  \nrvn                       0.0  \nar                        0.0  \ntusd                      0.0  \npax                       0.0  \niost                      0.0  \n\n[100 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>market_cap</th>\n      <th>name</th>\n      <th>genesis_date</th>\n      <th>market_cap_rank</th>\n      <th>hashing_algorithm</th>\n      <th>coingecko_rank</th>\n      <th>coingecko_score</th>\n      <th>developer_score</th>\n      <th>community_score</th>\n      <th>liquidity_score</th>\n      <th>public_interest_score</th>\n    </tr>\n    <tr>\n      <th>symbol</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>btc</th>\n      <td>bitcoin</td>\n      <td>943219001348</td>\n      <td>Bitcoin</td>\n      <td>2009-01-03</td>\n      <td>1</td>\n      <td>SHA-256</td>\n      <td>2</td>\n      <td>79.223</td>\n      <td>98.887</td>\n      <td>65.771</td>\n      <td>100.158</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>eth</th>\n      <td>ethereum</td>\n      <td>444995246879</td>\n      <td>Ethereum</td>\n      <td>2015-07-30</td>\n      <td>2</td>\n      <td>Ethash</td>\n      <td>3</td>\n      <td>77.163</td>\n      <td>97.176</td>\n      <td>62.612</td>\n      <td>98.968</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>bnb</th>\n      <td>binancecoin</td>\n      <td>92441015166</td>\n      <td>Binance Coin</td>\n      <td>2017-07-08</td>\n      <td>3</td>\n      <td>None</td>\n      <td>5</td>\n      <td>67.775</td>\n      <td>73.253</td>\n      <td>66.032</td>\n      <td>83.296</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>xrp</th>\n      <td>ripple</td>\n      <td>60573262757</td>\n      <td>XRP</td>\n      <td>None</td>\n      <td>4</td>\n      <td>None</td>\n      <td>9</td>\n      <td>65.306</td>\n      <td>71.120</td>\n      <td>54.187</td>\n      <td>87.663</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>usdt</th>\n      <td>tether</td>\n      <td>58147824102</td>\n      <td>Tether</td>\n      <td>None</td>\n      <td>5</td>\n      <td>None</td>\n      <td>151</td>\n      <td>41.650</td>\n      <td>0.000</td>\n      <td>10.676</td>\n      <td>107.153</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>rvn</th>\n      <td>ravencoin</td>\n      <td>1186666191</td>\n      <td>Ravencoin</td>\n      <td>2018-01-03</td>\n      <td>96</td>\n      <td>x16r</td>\n      <td>39</td>\n      <td>54.848</td>\n      <td>68.040</td>\n      <td>46.428</td>\n      <td>61.649</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ar</th>\n      <td>arweave</td>\n      <td>1184818634</td>\n      <td>Arweave</td>\n      <td>None</td>\n      <td>96</td>\n      <td>None</td>\n      <td>672</td>\n      <td>25.543</td>\n      <td>0.000</td>\n      <td>9.027</td>\n      <td>42.365</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>tusd</th>\n      <td>true-usd</td>\n      <td>1177644257</td>\n      <td>TrueUSD</td>\n      <td>2018-03-05</td>\n      <td>98</td>\n      <td>None</td>\n      <td>131</td>\n      <td>43.754</td>\n      <td>62.725</td>\n      <td>8.702</td>\n      <td>55.802</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>pax</th>\n      <td>paxos-standard</td>\n      <td>1172924271</td>\n      <td>Paxos Standard</td>\n      <td>None</td>\n      <td>99</td>\n      <td>None</td>\n      <td>174</td>\n      <td>39.457</td>\n      <td>47.971</td>\n      <td>8.271</td>\n      <td>52.646</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>iost</th>\n      <td>iostoken</td>\n      <td>1128186332</td>\n      <td>IOST</td>\n      <td>2018-01-11</td>\n      <td>100</td>\n      <td>None</td>\n      <td>38</td>\n      <td>54.955</td>\n      <td>67.201</td>\n      <td>45.077</td>\n      <td>64.822</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found crypto/gtrends/google_trends_df\n",
      "symbol btc\n",
      "name Bitcoin\n",
      "google trends is not empty\n",
      "found btc in gtrends_df\n",
      "found btc in gtrends_df\n",
      "found category 7 in gtrends_df\n",
      "break\n",
      "symbol eth\n",
      "name Ethereum\n",
      "google trends is not empty\n",
      "found eth in gtrends_df\n",
      "found eth in gtrends_df\n",
      "found category 7 in gtrends_df\n",
      "break\n",
      "symbol bnb\n",
      "name Binance Coin\n",
      "symbol xrp\n",
      "name XRP\n",
      "google trends is not empty\n",
      "get data from google\n",
      "xrp/:2016-01-01 2016-01-31\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "dataframe empty, sleep 5 min\n",
      "Trying again in 450 minutes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-dfcc9f75c8fb>\u001B[0m in \u001B[0;36m_fetch_data\u001B[0;34m(pytrends, build_payload, timeframe)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m             \u001B[0mbuild_payload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeframe\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/ml4t_gpu/lib/python3.8/site-packages/pytrends/request.py\u001B[0m in \u001B[0;36mbuild_payload\u001B[0;34m(self, kw_list, cat, timeframe, geo, gprop)\u001B[0m\n\u001B[1;32m    168\u001B[0m         \u001B[0;31m# get tokens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 169\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    170\u001B[0m         \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/ml4t_gpu/lib/python3.8/site-packages/pytrends/request.py\u001B[0m in \u001B[0;36m_tokens\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    174\u001B[0m         \u001B[0;31m# make the request and parse the returned json\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 175\u001B[0;31m         widget_dict = self._get_data(\n\u001B[0m\u001B[1;32m    176\u001B[0m             \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mTrendReq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGENERAL_URL\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/envs/ml4t_gpu/lib/python3.8/site-packages/pytrends/request.py\u001B[0m in \u001B[0;36m_get_data\u001B[0;34m(self, url, method, trim_chars, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m             \u001B[0;31m# error\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m             raise exceptions.ResponseError(\n\u001B[0m\u001B[1;32m    146\u001B[0m                 \u001B[0;34m'The request failed: Google returned a '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mResponseError\u001B[0m: The request failed: Google returned a response with code 429.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-bbcf3336a1c1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"get data from google\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;31m# if not break due to already aquired data:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             google_trend = get_hourly_data(search_term, start_year=2016, stop_year=2021,\n\u001B[0m\u001B[1;32m     37\u001B[0m                                            cat=category, wait_time=10,)\n\u001B[1;32m     38\u001B[0m             \u001B[0mgoogle_trend\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'symbol'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msymbol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-dfcc9f75c8fb>\u001B[0m in \u001B[0;36mget_hourly_data\u001B[0;34m(search_term, start_year, start_month, start_day, stop_year, stop_month, stop_day, geo, tz, cat, verbose, clean, wait_time, shift_hourly)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m     \u001B[0mdaily\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_daily_gtrends\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuild_payload\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgeo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpytrends\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msearch_term\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwait_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m     \u001B[0mmonthly\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_monthly_gtrends\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuild_payload\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpytrends\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msearch_term\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-dfcc9f75c8fb>\u001B[0m in \u001B[0;36mget_daily_gtrends\u001B[0;34m(build_payload, geo, pytrends, search_term, start_date, stop_date, verbose, wait_time)\u001B[0m\n\u001B[1;32m    257\u001B[0m         \u001B[0mtimeframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_timeframe_daily_resolution\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcurrent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlastDateOfMonth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    258\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'{search_term}/{geo}:{timeframe}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 259\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_fetch_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpytrends\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuild_payload\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    260\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         result.rename(columns={search_term: 'interest_daily_raw',\n",
      "\u001B[0;32m<ipython-input-12-dfcc9f75c8fb>\u001B[0m in \u001B[0;36m_fetch_data\u001B[0;34m(pytrends, build_payload, timeframe)\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0mwait_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m300\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mattempts\u001B[0m  \u001B[0;31m# Start with 0 due to timeouts\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Trying again in {wait_time / 60:.0f} minutes.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m             \u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwait_time\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m             \u001B[0mattempts\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ranked_market = market.sort_values(by='market_cap_rank')\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store_key = 'crypto/gtrends/google_trends_df'\n",
    "    if store_key in store:\n",
    "        google_trends_df = store[store_key]\n",
    "\n",
    "        print(f\"found {store_key}\")\n",
    "    else:\n",
    "        google_trends_df = pd.DataFrame()\n",
    "        print(f\"{store_key} not found. \")\n",
    "\n",
    "\n",
    "\n",
    "# Printing Name and AvgBill. In this case, \"x\" is a series with index of column names\n",
    "for index, contents in ranked_market.iterrows():\n",
    "    symbol = contents.name\n",
    "    symbol_name = contents['name']\n",
    "    print(\"symbol {}\\nname {}\".format(symbol, symbol_name))\n",
    "    search_terms = [contents.name, contents['name']]\n",
    "    category = 7 # finance\n",
    "    if symbol in prices_symbols:\n",
    "        for search_term in search_terms:\n",
    "            if not google_trends_df.empty:\n",
    "                print(\"google trends is not empty\")\n",
    "                if symbol in google_trends_df.index.get_level_values('symbol'):\n",
    "                    print(\"found {} in gtrends_df\".format(symbol))\n",
    "                    if search_term in google_trends_df.index.get_level_values('search_term'):\n",
    "                            print(\"found {} in gtrends_df\".format(search_term))\n",
    "                            if category in google_trends_df.index.get_level_values('google_category'):\n",
    "                                print(\"found category {} in gtrends_df\".format(category))\n",
    "                                print(\"break\")\n",
    "                                break\n",
    "            print(\"get data from google\")\n",
    "            # if not break due to already aquired data:\n",
    "            google_trend = get_hourly_data(search_term, start_year=2016, stop_year=2021,\n",
    "                                           cat=category, wait_time=10,)\n",
    "            google_trend['symbol'] = symbol\n",
    "            google_trend.set_index('symbol', inplace=True, append=True)\n",
    "\n",
    "            print(\"resulting google_trend: \\n{}\".format(google_trend))\n",
    "\n",
    "            google_trends_df = pd.concat([google_trends_df, google_trend])\n",
    "            print(\"google_trends_df: \\n{}\".format(google_trends_df))\n",
    "\n",
    "\n",
    "            with pd.HDFStore(DATA_STORE) as store:\n",
    "                store.put(store_key, google_trends_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests for making a library"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "google_trend = get_hourly_data('AMG Mercedes', start_year=2021, stop_year=2021, stop_month=2, stop_day=28,\n",
    "                               # cat=7,\n",
    "                               clean=False)\n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(24, 16))\n",
    "\n",
    "google_trend.ffill(inplace=True)\n",
    "\n",
    "google_trend.plot(ax=ax1)\n",
    "fig1.savefig('monthly_daily_interest.svg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "google_trend_shifted = get_hourly_data('AMG Mercedes', start_year=2021, stop_year=2021, stop_month=2, stop_day=28,\n",
    "                               clean=False, shift_hourly=True)\n",
    "\n",
    "google_trend_shifted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make sure trends are aligned by testing for the difference of a differently sampled daily period \n",
    "(already visually confirmed)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_keep = ['interest_daily', 'interest_hourly']\n",
    "joined_gtrends = google_trend[cols_keep].join(google_trend_shifted[cols_keep]*1.01, rsuffix='_shifted')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(32,20))\n",
    "joined_gtrends.plot(ax=ax)\n",
    "fig.savefig('shifted_validation.svg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-ceiling",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(24,16))\n",
    "\n",
    "google_trend.plot(ax=ax1)\n",
    "fig.savefig('monthly_daily_interest.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-mistress",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "google_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "google_trend_shifted.tail(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-canberra",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}