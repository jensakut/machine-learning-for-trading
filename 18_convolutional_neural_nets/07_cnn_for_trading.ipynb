{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Trading - Part 3: Training and Evaluating a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exploit the grid-like structure of time-series data, we can use CNN architectures for univariate and multivariate time series. In the latter case, we consider different time series as channels, similar to the different color signals.\n",
    "\n",
    "An alternative approach converts a time series of alpha factors into a two-dimensional format to leverage the ability of CNNs to detect local patterns. [Sezer and Ozbayoglu (2018)](https://www.researchgate.net/publication/324802031_Algorithmic_Financial_Trading_with_Deep_Convolutional_Neural_Networks_Time_Series_to_Image_Conversion_Approach) propose CNN-TA, which computes 15 technical indicators for different intervals and uses hierarchical clustering (see Chapter 13, Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning) to locate indicators that behave similarly close to each other in a two-dimensional grid.\n",
    "\n",
    "The authors train a CNN similar to the CIFAR-10 example we used earlier to predict whether to buy, hold, or sell an asset on a given day. They compare the CNN performance to \"buy-and-hold\" and other models and find that it outperforms all alternatives using daily price series for Dow 30 stocks and the nine most-traded ETFs over the 2007-2017 time period.\n",
    "\n",
    "The section on *CNN for Trading* consists of three notebooks that experiment with this approach using daily US equity price data. They demonstrate \n",
    "1. How to compute relevant financial features\n",
    "2. How to convert a similar set of indicators into image format and cluster them by similarity\n",
    "3. How to train a CNN to predict daily returns and evaluate a simple long-short strategy based on the resulting signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to design, train, and evaluate a CNN following the steps outlined in the\n",
    "previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:49.514673Z",
     "start_time": "2021-02-23T20:16:49.511803Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:51.292791Z",
     "start_time": "2021-02-23T20:16:49.519064Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:51.352783Z",
     "start_time": "2021-02-23T20:16:51.293765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:51.365039Z",
     "start_time": "2021-02-23T20:16:51.353993Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from utils import MultipleTimeSeriesCV, format_time, calculate_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:51.370474Z",
     "start_time": "2021-02-23T20:16:51.366139Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:51.383882Z",
     "start_time": "2021-02-23T20:16:51.371356Z"
    }
   },
   "outputs": [],
   "source": [
    "size = 15\n",
    "lookahead = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:51.391957Z",
     "start_time": "2021-02-23T20:16:51.384923Z"
    }
   },
   "outputs": [],
   "source": [
    "results_path = Path('results', 'cnn_for_trading')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:55.361338Z",
     "start_time": "2021-02-23T20:16:51.392807Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "features = pd.read_feather('img_data.ftr').set_index(['date', 'ticker']).sort_index()\n",
    "\n",
    "targets = pd.read_feather('targets.ftr').set_index(['date', 'ticker']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:55.400785Z",
     "start_time": "2021-02-23T20:16:55.362774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 949852 entries, (Timestamp('2013-04-01 01:00:00'), 'btc-usd') to (Timestamp('2021-05-28 23:00:00'), 'zec-usd')\n",
      "Columns: 225 entries, 11_ADOSC to 04_BBL\n",
      "dtypes: float32(225)\n",
      "memory usage: 823.3 MB\n"
     ]
    }
   ],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:55.437458Z",
     "start_time": "2021-02-23T20:16:55.401927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 949852 entries, (Timestamp('2013-04-01 01:00:00'), 'btc-usd') to (Timestamp('2021-05-28 23:00:00'), 'zec-usd')\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   r12_fwd     949468 non-null  float32\n",
      " 1   r12dec_fwd  939879 non-null  float64\n",
      " 2   r24_fwd     949084 non-null  float32\n",
      " 3   r24dec_fwd  939633 non-null  float64\n",
      "dtypes: float32(2), float64(2)\n",
      "memory usage: 29.7 MB\n"
     ]
    }
   ],
   "source": [
    "targets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:57.893458Z",
     "start_time": "2021-02-23T20:16:55.439475Z"
    }
   },
   "outputs": [],
   "source": [
    "outcome = f'r{lookahead:02}_fwd'\n",
    "features = features.join(targets[[outcome]]).dropna()\n",
    "target = features[outcome]\n",
    "features = features.drop(outcome, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 947668 entries, (Timestamp('2013-04-02 21:00:00'), 'btc-usd') to (Timestamp('2021-05-27 23:00:00'), 'zec-usd')\n",
      "Columns: 225 entries, 11_ADOSC to 04_BBL\n",
      "dtypes: float32(225)\n",
      "memory usage: 821.4 MB\n"
     ]
    }
   ],
   "source": [
    "features.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again closely follow the authors in creating a CNN with 2 convolutional layers with kernel size 3 and 16 and 32 filters, respectively, followed by a max pooling layer of size 2. \n",
    "\n",
    "We flatten the output of the last stack of filters and connect the resulting 1,568 outputs to a dense layer of size 32, applying 25 and 50 percent dropout probability to the incoming and outcoming connections to mitigate overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:57.897808Z",
     "start_time": "2021-02-23T20:16:57.894295Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(filter1=16, act1='relu', filter2=32, act2='relu', do1=.25, do2=.5, dense=32):\n",
    "    input_shape = (size, size, 1)\n",
    "    cnn = Sequential([\n",
    "        Conv2D(filters=filter1,\n",
    "               kernel_size=3,\n",
    "               padding='same',\n",
    "               activation=act1,\n",
    "               input_shape=input_shape,\n",
    "               name='CONV1'),\n",
    "        Conv2D(filters=filter2,        \n",
    "               kernel_size=3,  \n",
    "               padding='same',\n",
    "               activation=act2,\n",
    "               name='CONV2'),\n",
    "        MaxPooling2D(pool_size=2, name='POOL2'),\n",
    "        Dropout(do1, name='DROP1'),\n",
    "        Flatten(name='FLAT1'),\n",
    "        Dense(dense, activation='relu', name='FC1'),\n",
    "        Dropout(do2, name='DROP2'),\n",
    "        Dense(1, activation='linear', name='FC2')\n",
    "    ])\n",
    "    cnn.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, \n",
    "                                                  momentum=0.9, \n",
    "                                                  nesterov=False, \n",
    "                                                  name='SGD'),\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:58.298472Z",
     "start_time": "2021-02-23T20:16:57.898871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "CONV1 (Conv2D)               (None, 15, 15, 16)        160       \n",
      "_________________________________________________________________\n",
      "CONV2 (Conv2D)               (None, 15, 15, 32)        4640      \n",
      "_________________________________________________________________\n",
      "POOL2 (MaxPooling2D)         (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "DROP1 (Dropout)              (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "FLAT1 (Flatten)              (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 32)                50208     \n",
      "_________________________________________________________________\n",
      "DROP2 (Dropout)              (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 55,041\n",
      "Trainable params: 55,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = make_model()\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cross-validate the model with the MutipleTimeSeriesCV train and validation set index generator introduced in Chapter 7, Linear Models – From Risk Factors to Return Forecasts. We provide 5 years of trading days during the training period in batches of 64 random samples and validate using the subsequent 3 months, covering the years 2014-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:58.304380Z",
     "start_time": "2021-02-23T20:16:58.300011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unstack ticker\n",
      "Duplicate index: \n"
     ]
    },
    {
     "data": {
      "text/plain": "(35437, 2953)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 12\n",
    "train_test_ratio = 12\n",
    "\n",
    "train_period_length, test_period_length = calculate_splits(features, lookahead=lookahead,\n",
    "                                                           train_test_ratio=train_test_ratio,\n",
    "                                                           n_splits=n_splits,\n",
    "                                                           unstack='ticker')\n",
    "\n",
    "train_period_length, test_period_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:58.311610Z",
     "start_time": "2021-02-23T20:16:58.305432Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = MultipleTimeSeriesCV(n_splits=n_splits,\n",
    "                          train_period_length=train_period_length,\n",
    "                          test_period_length=test_period_length,\n",
    "                          lookahead=lookahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the features to the range [-1, 1] and again use NumPy's .reshape() method to create the requisite format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:58.320139Z",
     "start_time": "2021-02-23T20:16:58.312511Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_valid_data(X, y, train_idx, test_idx):\n",
    "    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_val = scaler.transform(x_val)\n",
    "    return (x_train.reshape(-1, size, size, 1), y_train,\n",
    "            x_val.reshape(-1, size, size, 1), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:58.331324Z",
     "start_time": "2021-02-23T20:16:58.321582Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T20:16:58.339491Z",
     "start_time": "2021-02-23T20:16:58.332279Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = results_path / f'lookahead_{lookahead:02d}'\n",
    "if not checkpoint_path.exists():\n",
    "    checkpoint_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation follow the process laid out in Chapter 17, Deep Learning for Trading, relying on checkpointing to store weights after each epoch and generate predictions for the best-performing iterations without the need for costly retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:44:20.456311Z",
     "start_time": "2021-02-23T20:20:45.674180Z"
    },
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:23 | 01 | 01 |  0.0574 |  0.0588\n",
      "00:00:43 | 01 | 02 |  0.0749 |  0.0731\n",
      "00:01:03 | 01 | 03 |  0.0770 |  0.0734\n",
      "00:01:24 | 01 | 04 |  0.0864 |  0.0862\n",
      "00:01:44 | 01 | 05 |  0.0889 |  0.0894\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "ic  = []\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(features)):\n",
    "    X_train, y_train, X_val, y_val = get_train_valid_data(features, target, train_idx, test_idx)\n",
    "    preds = y_val.to_frame('actual')\n",
    "    r = pd.DataFrame(index=y_val.index.unique(level='date')).sort_index()\n",
    "    model = make_model(filter1=16, act1='relu', filter2=32, \n",
    "                       act2='relu', do1=.25, do2=.5, dense=32)\n",
    "    best_mean = best_median = -np.inf\n",
    "    for epoch in range(25):         \n",
    "        model.fit(X_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=epoch + 1,\n",
    "                  initial_epoch=epoch,\n",
    "                  verbose=0, shuffle=True)\n",
    "        model.save_weights((checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n",
    "        preds[epoch] = model.predict(X_val).squeeze()\n",
    "        r[epoch] = preds.groupby(level='date').apply(lambda x: spearmanr(x.actual, x[epoch])[0]).to_frame(epoch)\n",
    "        print(f'{format_time(time()-start)} | {fold + 1:02d} | {epoch + 1:02d} | {r[epoch].mean():7.4f} | {r[epoch].median():7.4f}')\n",
    "    ic.append(r.assign(fold=fold))\n",
    "ic = pd.concat(ic)\n",
    "ic.to_csv(checkpoint_path / 'ic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:52:07.839243Z",
     "start_time": "2021-02-23T21:52:07.485960Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ic.groupby('fold').mean().boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:52:08.025753Z",
     "start_time": "2021-02-23T21:52:07.840249Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ic.groupby('fold').mean().mean().sort_index().plot.bar(rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:52:09.854588Z",
     "start_time": "2021-02-23T21:52:08.026941Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(h_neg=20, h_pos=210)\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sns.heatmap(ic.groupby('fold').mean().mul(100), ax=ax, center=0, cmap=cmap, annot=True, fmt='.1f')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model's predictive accuracy, we compute the daily information coefficient (IC) for the validation set like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:52:13.369122Z",
     "start_time": "2021-02-23T21:52:13.366095Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_predictions(epoch):\n",
    "    predictions = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(features)):\n",
    "        X_train, y_train, X_val, y_val = get_train_valid_data(features, target, train_idx, test_idx)\n",
    "        preds = y_val.to_frame('actual')\n",
    "        model = make_model(filter1=16, act1='relu', filter2=32, \n",
    "                       act2='relu', do1=.25, do2=.5, dense=32)\n",
    "        status = model.load_weights((checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())\n",
    "        status.expect_partial()\n",
    "        predictions.append(pd.Series(model.predict(X_val).squeeze(), index=y_val.index))\n",
    "    return pd.concat(predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:54:26.067314Z",
     "start_time": "2021-02-23T21:52:13.636708Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for i, epoch in enumerate(ic.drop('fold', axis=1).mean().nlargest(5).index):\n",
    "    preds[i] = generate_predictions(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:54:26.654724Z",
     "start_time": "2021-02-23T21:54:26.068767Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preds).sort_index().reset_index().to_feather('predictions.ftr')\n",
    "\n",
    "#with pd.HDFStore(results_path / 'predictions.h5') as store:\n",
    "#    store.put('predictions', pd.DataFrame(preds).sort_index())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}